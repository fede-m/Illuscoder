{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fede-m/Illuscoder/blob/main/IllusCoder_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWkoIc7zPkEg"
      },
      "outputs": [],
      "source": [
        "# Things to install\n",
        "\n",
        "# 1) Install ngrok to start Flask application\n",
        "!pip install pyngrok\n",
        "\n",
        "# 2) Summarize text\n",
        "!pip install sentencepiece\n",
        "\n",
        "# 3) Named Entity Recognition\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "# 4) Generate images\n",
        "!pip install diffusers==0.8.0 transformers ftfy\n",
        "!pip install accelerate\n",
        "\n",
        "# 5) Modify images\n",
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyHEDlUXkRjq"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "\n",
        "# Text summarizer\n",
        "from transformers import pipeline\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import random\n",
        "\n",
        "# Named Entity Recognition\n",
        "import spacy\n",
        "\n",
        "# Stable Diffusion Models\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import DPMSolverMultistepScheduler\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "#check for gpu\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.device(\"cuda\")\n",
        "else:\n",
        "    device_name = torch.device('cpu')\n",
        "\n",
        "\n",
        "#Image modification\n",
        "from PIL import Image, ImageFont, ImageDraw\n",
        "import textwrap\n",
        "\n",
        "# Mount google colab to have access to the HTML, CSS and JavaScript files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58TrPxUilPtc"
      },
      "outputs": [],
      "source": [
        "# Text summarizer\n",
        "\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "#tokenizer = PegasusTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "#model = PegasusForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Named Entity Recognition\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "\n",
        "# Image Generation\n",
        "pipe = DiffusionPipeline.from_pretrained(\"naclbit/trinart_characters_19.2m_stable_diffusion_v1\")\n",
        "#pipeline = DiffusionPipeline.from_pretrained(\"eimiss/EimisAnimeDiffusion_1.0v\")\n",
        "\n",
        "# Scheduler for Diffusion Model ---> fastest by now is DPMSolverMultistepScheduler (Stable_Diffusions_2)\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "#pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "\n",
        "# Generator --> Use the manual_seed attribute to select and fix a seed. This helps ensuring coherence in the style of the outputs\n",
        "generator = torch.Generator().manual_seed(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jFbG6UwNKv0"
      },
      "outputs": [],
      "source": [
        "# Define a palette according to the genre to ensure coherence in style\n",
        "def set_mood(genre):\n",
        "  palettes = {\"fantasy\": \"light blue, grey and white\", \"adventure\":\"brown and orange\", \"romantic\":\"red, rose, bordeaux\", \"horror\":\"black, red, gray, white\" }\n",
        "  return palettes[genre]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfrCJY-7k1HJ"
      },
      "outputs": [],
      "source": [
        "# Summarize the text of the chapter to get only the most important parts of the story\n",
        "\n",
        "def summarize_text(chapter, summarizer):\n",
        "\n",
        "  '''\n",
        "  param: chapter --> string with the current chapter text\n",
        "  param: summarizer --> summarization model (BART)\n",
        "\n",
        "  output: list of max 3 summarized sentences\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Clean text (replace /n etc.)\n",
        "  chapter.replace(\"\\n\", \"\")\n",
        "\n",
        "  # Generate summary of the chapter\n",
        "  summary = summarizer(chapter, max_length=130, min_length=30, do_sample=False)\n",
        "  sents = summary[0]['summary_text']\n",
        "\n",
        "  # Pick only 3 sentences (to generate only 3 images for every chapter)\n",
        "  sentences = sent_tokenize(sents)\n",
        "  if len(sentences) > 3:\n",
        "    sentences = random.sample(sentences, 3)\n",
        "\n",
        "  return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdGYVKuzo0ro"
      },
      "outputs": [],
      "source": [
        "# Find people involved in the story, places and actions\n",
        "def named_entity_recognition(sentence, nlp):\n",
        "\n",
        "  doc = nlp(sentence)\n",
        "  per_entities = []\n",
        "  loc_entities = []\n",
        "  actions = [\"\",\"\", \"\", \"\"]      #(verb, direct object, adverb, attribute)\n",
        "  persons = []\n",
        "\n",
        "  print(sentence)\n",
        "\n",
        "  # extract characters and locations\n",
        "  for ent in doc.ents:\n",
        "\n",
        "    if ent.label_ == \"PERSON\":\n",
        "      persons.append(ent)\n",
        "    if ent.label_ == (\"LOC\" or \"GPE\" or \"FAC\"):\n",
        "      loc_entities.append(ent)\n",
        "  print(persons)\n",
        "\n",
        "  # extract actions\n",
        "  for token in doc:\n",
        "    if token.dep_ == \"nsubj\":\n",
        "      if len(persons)== 0:\n",
        "        per_entities.append(token)\n",
        "      else:                         #Check if Subject is a detected Person, include otherwise\n",
        "          is_person = False\n",
        "          for p in persons:\n",
        "            if token in p:\n",
        "              is_person = True\n",
        "              per_entities.append(p)\n",
        "          if is_person == False:\n",
        "            per_entities.append(token)\n",
        "\n",
        "\n",
        "    # Added negation\n",
        "    if token.dep_ == \"neg\":\n",
        "      actions[0] = token\n",
        "    if token.dep_ == \"ROOT\":\n",
        "      actions[1] = token\n",
        "    if token.dep_ == \"dobj\":\n",
        "      actions[3] = token\n",
        "    if token.dep_ == \"acomp\" and actions[1].lemma_==\"be\":\n",
        "      actions[2] = token\n",
        "    if token.dep_ == \"attr\" and actions[1].lemma_==\"be\":\n",
        "      actions[3] = token\n",
        "\n",
        "    # for p in persons:\n",
        "    #   if p not in per_entities and p not in actions[3]:\n",
        "    #     per_entities.append(p)\n",
        "\n",
        "\n",
        "  return per_entities, loc_entities, actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lTsVgcezIPx"
      },
      "outputs": [],
      "source": [
        "# Prepare the prompts to feed the model with\n",
        "def prepare_prompts(sentences, nlp, characters, time, genre):\n",
        "  prompts = []\n",
        "  # Set the colors for the mood\n",
        "  palette = set_mood(genre)\n",
        "  for sent in sentences:\n",
        "    curr_prompt = \"\"\n",
        "    char_entities, loc_entities, actions = named_entity_recognition(sent, nlp)\n",
        "\n",
        "    # Add characters to prompt\n",
        "    for character in char_entities:\n",
        "\n",
        "      is_character = False\n",
        "      for char in characters:\n",
        "        if str(character) in char[\"name\"]:\n",
        "          curr_prompt += char[\"gender\"] + \", \" + char[\"hairColor\"] + \", \" + char[\"eyesColor\"] + \", \"\n",
        "          is_character = True\n",
        "      if not is_character:\n",
        "        curr_prompt += str(character) + \", \"\n",
        "\n",
        "    # Add actions to prompt\n",
        "    for action in actions:\n",
        "      is_character = False\n",
        "      for char in characters:\n",
        "        if str(action) in char[\"name\"].split():\n",
        "          curr_prompt += char[\"gender\"] + \", \" + char[\"hairColor\"] + \", \" + char[\"eyesColor\"] + \", \"\n",
        "          is_character = True\n",
        "      if not is_character:\n",
        "        curr_prompt += str(action) + \" \"\n",
        "\n",
        "    # Add places to prompt\n",
        "    for place in loc_entities:\n",
        "          curr_prompt += \", \" + str(place)\n",
        "    curr_prompt += \", \" +time\n",
        "\n",
        "    # Add palette\n",
        "    curr_prompt += \", \" + palette\n",
        "\n",
        "    prompts.append(curr_prompt)\n",
        "  return prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWggR4SvgGo4"
      },
      "outputs": [],
      "source": [
        "def generate_images(prompts):\n",
        "  images = []\n",
        "  n_prompt = \"lowres, NSFW, bad anatomy, bad hands, text, error, missing fingers, cropped, jpeg artifacts, worst quality, low quality, signature, watermark, blurry, deformed, extra ears, deformed, disfigured, mutation, censored\"\n",
        "\n",
        "  # Generate an image for each prompt\n",
        "  for p in prompts:\n",
        "    img =pipe(prompt=p, negative_prompt=n_prompt,generator=generator, num_inference_steps = 7).images[0]\n",
        "    img.show()\n",
        "\n",
        "    # Check if image is NSFW (in this case the image is completely black) and if so, generate a new image\n",
        "    good_image = img.getbbox()\n",
        "    while not good_image:\n",
        "      print(\"NSFW content\")\n",
        "      img =pipe(prompt=p, negative_prompt=n_prompt,generator=generator, num_inference_steps = 7).images[0]\n",
        "      good_image = img.getbbox()\n",
        "    images.append(img)\n",
        "  return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpToLNQN71pM"
      },
      "outputs": [],
      "source": [
        "def add_text(img, sent):\n",
        "  \"\"\"Adds a sentence on a white space at the bottom of the picture.\"\"\"\n",
        "  draw = ImageDraw.Draw(img)\n",
        "  sent = textwrap.fill(sent, img.size[0]//6)      # Cut Sentence to the length of the picture\n",
        "  text_h  = draw.textsize(sent)[1]       #get height of text\n",
        "  new_size = (img.size[0], img.size[1]+text_h)\n",
        "  new_image = Image.new('RGB', new_size, (255, 255, 255)) #slightly larger white background\n",
        "  new_image.paste(img)\n",
        "\n",
        "  #add text into white area\n",
        "  h = new_image.size[1]                 #get height of image\n",
        "  ImageDraw.Draw(new_image).text(( 0, h-text_h), sent, (0,0,0))\n",
        "\n",
        "\n",
        "  return new_image\n",
        "\n",
        "def make_gif(images, curr_chapter):\n",
        "  \"\"\"takes list of images and saves them as a gif\"\"\"\n",
        "  frame_one = images[0]\n",
        "  w, h = images[0].size\n",
        "  ims = [i.resize((w, h)) for i in images]\n",
        "\n",
        "  frame_one.save(\"/content/drive/MyDrive/app/static/images/img-\"+curr_chapter+\".gif\", format=\"GIF\", append_images=ims, save_all=True, duration=5000, loop=0)\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHU0Bnlr8hqJ"
      },
      "outputs": [],
      "source": [
        "def handler(story_prompt):\n",
        "\n",
        "  # 1) summarize chapter text\n",
        "  summarized_text = summarize_text(story_prompt[\"chapter\"], summarizer)\n",
        "\n",
        "  # 2) prepare prompts --> extract the characters description, the background etc.\n",
        "  prompts = prepare_prompts(summarized_text, nlp, story_prompt[\"characters\"], story_prompt[\"time\"], story_prompt[\"genre\"])\n",
        "\n",
        "  # 3) generate images (one image for each prompt)\n",
        "\n",
        "  images = generate_images(prompts)\n",
        "\n",
        "  # 4) add text to image\n",
        "  images_with_text = []\n",
        "  for img, sent in zip(images, summarized_text):\n",
        "    images_with_text.append(add_text(img, sent))\n",
        "\n",
        "  # 5) store them in /images folder on Google Drive\n",
        "\n",
        "  make_gif(images_with_text, story_prompt[\"currChapter\"])\n",
        "\n",
        "  # Return the url with the GIF on google drive that can be added at the src element of the image on Javascript (does not work)\n",
        "  img_name = \"img-\"+ story_prompt[\"currChapter\"]+\".gif\"\n",
        "\n",
        "  return img_name\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from flask import Flask, render_template, request, jsonify, redirect\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import io\n",
        "\n",
        "# Set paths for HTML, CSS and Javascript files\n",
        "IMAGES_FOLDER = os.path.join('static', 'images')\n",
        "\n",
        "app = Flask(__name__, template_folder=\"/content/drive/MyDrive/app/templates\", static_folder=\"/content/drive/MyDrive/app/static\")\n",
        "app.config['UPLOAD_FOLDER'] = IMAGES_FOLDER\n",
        "# Set ngrok token to start the server\n",
        "ngrok_token = os.environ[\"NGROK_TOKEN\"]\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Get a public URL that can be accessed from an external user\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(public_url)\n",
        "\n",
        "# Show the HTML template\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "# Route to generate the image\n",
        "@app.route('/generate_image', methods=['GET', 'POST'])\n",
        "def generate_image():\n",
        "  unpacked_json = request.get_json(force=True) # unpack the request in a python dictionary\n",
        "\n",
        "  img_url = handler(unpacked_json) # Start the process\n",
        "  img_name= \"img-\"+unpacked_json[\"currChapter\"]+\".gif\"\n",
        "  resp = jsonify(img_url = img_url, success=True)\n",
        "  image_path = os.path.join(app.config['UPLOAD_FOLDER'], img_name)\n",
        "\n",
        "  return render_template('index.html', gif_name=image_path)\n",
        "\n",
        "\n",
        "app.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
